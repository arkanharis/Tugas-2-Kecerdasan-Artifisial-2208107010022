# -*- coding: utf-8 -*-
"""Tugas-2-Kecerdasan-Artifisial-2208107010022

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qYji1_X1SK2xdXiJP840iugcOBXsvoSG
"""

#Import library yyang diperlukan
import math, re, os, warnings
import numpy as np
import pandas as pd

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import preprocessing
from tensorflow.keras.models import Model
from tensorflow.keras.models import load_model


import matplotlib.pyplot as plt
import cv2
import seaborn as sns
sns.set_style('whitegrid')

from sklearn.model_selection import train_test_split

from tensorflow.keras.preprocessing import image_dataset_from_directory

import time

import warnings
warnings.filterwarnings('ignore')

print("TensorFlow version:", tf.__version__)

"""### Import data

**Menetapkan default untuk matplotlib**
"""

# Menetapkan default untuk matplotlib
plt.rc('figure', autolayout=True)
plt.rc('axes', labelweight='bold', labelsize='large',
       titleweight='bold', titlesize=18, titlepad=10)
plt.rc('image', cmap='magma')
warnings.filterwarnings("ignore") # agar output bersih

# menetapkan path untuk dataset

train_data_file_path = '/content/train-00000-of-00001-c08a401c53fe5312.parquet'
test_data_file_path = '/content/train-00000-of-00001-c08a401c53fe5312.parquet'

# membaca dataset

train_data = pd.read_parquet(train_data_file_path)
test_data = pd.read_parquet(test_data_file_path)

"""Mengubah format byte menjadi img"""

def dict_to_image(image_dict):
    if isinstance(image_dict, dict) and 'bytes' in image_dict:
        byte_string = image_dict['bytes']
        nparr = np.frombuffer(byte_string, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        return img

train_data['image'] = train_data['image'].apply(dict_to_image)
test_data['image'] = test_data['image'].apply(dict_to_image)

train_data.head()

"""**Membagi training set dan val set**"""

X = train_data['image']
y = train_data['label']

X_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.75, random_state=10)

X_test = test_data['image']
y_test = test_data['label']

"""menggabungkan data kembali ke dalam satu dataframe"""

# Training data

X_train_df = pd.DataFrame(X_train)
y_train_df = pd.DataFrame(y_train)
train_df = pd.concat([X_train_df, y_train_df], axis=1)

# Validation data

X_val_df = pd.DataFrame(X_val)
y_val_df = pd.DataFrame(y_val)
val_df = pd.concat([X_val_df, y_val_df], axis=1)

# Test data
X_test_df = pd.DataFrame(X_test)
y_test_df = pd.DataFrame(y_test)
test_df = pd.concat([X_test_df, y_test_df], axis=1)

"""****

# **Eksplorasi Analisis Data**
"""

train_df.head()

"""**Kita bisa melakukan ekplorasi untuk beberapa gambar**"""

# Gambar training set
mri_images = train_df['image']

fig = plt.figure(figsize=(10, 7))

for i in range(0, len(mri_images), 1000):
    n = int((i / 1000) + 1)

    fig.add_subplot(2,2,n)
    plt.imshow(mri_images.iloc[i])

plt.show()

# Gambar test set
mri_images_test = test_df['image']

fig = plt.figure(figsize=(10, 7))

# Batasi jumlah gambar yang ditampilkan agar sesuai dengan jumlah subplot
for i in range(0, min(len(mri_images_test), 4) * 1000, 1000):
    n = int((i / 1000) + 1)

    fig.add_subplot(2, 2, n)  # Maksimal subplot adalah 4 (2x2)
    plt.imshow(mri_images_test.iloc[i])
    plt.axis('off')  # Hilangkan sumbu untuk estetika

plt.show()

"""Coba kita lihat jumlah data untuk setiap label nya"""

labels = train_data['label']
label_counts = pd.DataFrame(labels.value_counts())
label_counts.index = ['Non Demented', 'Very Mild Demented', 'Mild Demented', 'Moderate Demented']
label_counts

label_plot = label_counts.plot.bar()
plt.show()

"""terdapat ketidakseimbangan kelas yang sangat jelas. dengan sangat sedikit gambar Moderate Demented dan Mild Demented, akan sangat bermanfaat untuk melakukan augmentasi data guna meningkatkan ukuran kelas tersebut.

****

# **Preprocessing**

### Reshape gambar
"""

from tensorflow.keras.utils import to_categorical

# Define reshape function
def reshape_X_y(X, y):
    X_array = []
    for x in X:
        X_array.append(x)

    y = to_categorical(y)

    # Normalisation
    X_array = np.array(X_array)
    X_array = X_array / 255.0

    y = np.array(y)

    print(X_array.shape, y.shape)
    return X_array, y

# Training data
X_train_ds, y_train_ds = reshape_X_y(X_train, y_train)

# Validation data
X_val_ds, y_val_ds = reshape_X_y(X_val, y_val)

# Test data
X_test_ds, y_test_ds = reshape_X_y(X_test, y_test)

"""### Definisikan nama class"""

class_names = np.array(['Non Demented', 'Very Mild Demented', 'Mild Demented', 'Moderate Demented'])
print(class_names)

"""****

# **Pembuatan Model**

### Model: *Custom convolutional net*
"""

model = keras.Sequential([

    # First Block
    layers.Conv2D(kernel_size=3, filters=32, input_shape=([128, 128, 3]), activation='relu', padding='same'),
    layers.MaxPool2D(),

    # Second Block
    layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),
    layers.MaxPool2D(),

    # Classifier Head
    layers.Flatten(),
    layers.Dense(units=64, activation='relu'),
    layers.Dense(units=4, activation='softmax')
])

model.compile(optimizer=tf.keras.optimizers.Adam(epsilon=0.01),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

start = time.time()

history = model.fit(X_train_ds, y_train_ds,
                      steps_per_epoch=len(X_train_ds),
                      batch_size=32,
                      validation_data=(X_val_ds, y_val_ds),
                      validation_steps=len(X_val_ds),
                      epochs=40
)

print('Total waktu %s menit' % ((time.time() - start)/60))

model.summary()

history_plot = pd.DataFrame(history.history)
history_plot.head()

"""## **Evaluasi Model Awal**"""

plt.rcParams['figure.figsize'] = [4, 4]

"""**Validation loss**"""

history_plot.loc[:,['loss', 'val_loss']].plot()
plt.xlim(0, 40)
plt.ylim(0, 1)

"""**Validation Accuracy**"""

history_plot.loc[:,['accuracy', 'val_accuracy']].plot()
plt.xlim(0, 40)
plt.ylim(0, 1)

"""****

# **Eksperimen untuk meningkatkan performa model**

Model mengalami underfitting yang ditunjukkan oleh jarak besar antara kurva pelatihan dan kurva validasi.

#### Eksperimen 1: EarlyStopping and Precision

Saya akan bereksperimen dengan menggunakan EarlyStopping. Karena kelas yang tidak seimbang, menggunakan akurasi juga bukan pilihan yang baik, jadi saya akan menggunakan presisi sebagai metrik saya. Sebagai aturan umum, mengubah lebih dari satu parameter sekaligus tidak disarankan, namun penggunaan akurasi sebagai metrik kemungkinan besar tidak akan diteruskan, terlepas dari itu.
"""

# 1. Build experiment 1

model_1 = keras.Sequential([

    # First Block
    layers.Conv2D(kernel_size=3, filters=32, input_shape=([128, 128, 3]), activation='relu', padding='same'),
    layers.MaxPool2D(),


    # Second Block
    layers.Conv2D(filters=64, activation='relu', padding='same', kernel_size=3),
    layers.MaxPool2D(),

    # Output Layers
    layers.Flatten(),
    layers.Dense(units=64, activation='relu'),
    layers.Dense(units=4, activation='softmax')
])

# 2. Compile experiment 1

model_1.compile(optimizer=tf.keras.optimizers.Adam(),
                loss='categorical_crossentropy',
                metrics=['precision'])

# - Define Early Stopping

earlystop1_1 = keras.callbacks.EarlyStopping(monitor='val_loss',
                                           patience=10)

# 3. Fit experiment 1

history_1 = model_1.fit(X_train_ds, y_train_ds,
                steps_per_epoch=len(X_train_ds),
                validation_data=(X_val_ds, y_val_ds),
                validation_steps=len(X_val_ds),
                batch_size=32,
                epochs=100,
                callbacks=[earlystop1_1])

history_1_plot = pd.DataFrame(history_1.history)
history_1_plot.loc[: , ['loss', 'val_loss']].plot()
history_1_plot.loc[:, ['precision', 'val_precision']].plot()

"""#### Eksperimen 2: Mencari learning rate ideal

Kita dapat menggunakan callback scheduler laju pembelajaran untuk memvisualisasikan laju pembelajaran yang paling efektif.
"""

# 1. Build experiment 2

model_2 = keras.Sequential([

    # First Block
    layers.Conv2D(kernel_size=3, filters=32, input_shape=([128, 128, 3]), activation='relu', padding='same'),
    layers.MaxPool2D(),


    # Second Block
    layers.Conv2D(filters=64, activation='relu', padding='same', kernel_size=3),
    layers.MaxPool2D(),

    # Output Layers
    layers.Flatten(),
    layers.Dense(units=64, activation='relu'),
    layers.Dense(units=4, activation='softmax')
])

# 2. Compile experiment 2

model_2.compile(optimizer=tf.keras.optimizers.Adam(),
                loss='categorical_crossentropy',
                metrics=['precision'])

# - Define Early Stopping
earlystop1_2 = keras.callbacks.EarlyStopping(monitor='val_loss',
                                           patience=10)
# - Define Learnrate Scheduler
lrschedule1_2 = keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10**(epoch/20))



# 3. Fit experiment 2

history_2 = model_2.fit(X_train_ds, y_train_ds,
                batch_size=32,
                steps_per_epoch=len(X_train_ds),
                validation_data=(X_val_ds, y_val_ds),
                validation_steps=len(X_val_ds),
                epochs=100,
                callbacks=[earlystop1_1, lrschedule1_2])

len(history_2.history['loss'])

lrs = 1e-3 * (10**(tf.range(len(history_2.history['loss']))/20))
plt.figure(figsize=(8,4))
plt.semilogx(lrs, history_2.history['loss'])
plt.xlabel("Learning Rate")
plt.ylabel("Loss")
plt.title("Mencari learning rate ideal")

"""Sepertinya learning rate yang ideal adalah 0.002 - 0.0025, dan jumlah epoch yang ideal adalah 20.

#### Eksperimen 3: Turunkan learning rate untuk mengurangi spikes.
"""

import tensorflow as tf
import datetime

# 1. Build model (Model 3)
model_3 = keras.Sequential([
    layers.Conv2D(kernel_size=3, filters=32, input_shape=(128, 128, 3), activation='relu'),
    layers.MaxPool2D(),
    layers.Conv2D(filters=64, activation='relu', kernel_size=3),
    layers.MaxPool2D(),
    layers.Flatten(),
    layers.Dense(units=64, activation='relu'),
    layers.Dense(units=4, activation='softmax')
])

# 2. Compile model
lr1_3 = 0.002
epochs1_3 = 20
model_3.compile(optimizer=tf.keras.optimizers.Adam(epsilon=lr1_3),
                loss='categorical_crossentropy',
                metrics=['precision'])

# 3. Define Early Stopping
earlystop1_3 = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)

# 4. Define TensorBoard callback
log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)

# 5. Fit model with TensorBoard callback
history_3 = model_3.fit(X_train_ds, y_train_ds,
                        steps_per_epoch=len(X_train_ds),
                        batch_size=32,
                        validation_data=(X_val_ds, y_val_ds),
                        validation_steps=len(X_val_ds),
                        epochs=epochs1_3,
                        callbacks=[earlystop1_3, tensorboard_callback])

# Plot model
history_3_plot = pd.DataFrame(history_3.history)

plt.figure(figsize=(8,12))
history_3_plot.loc[: , ['loss', 'val_loss']].plot()
history_3_plot.loc[: , ['precision', 'val_precision']].plot()

#history1_plot.loc[: , ['loss', 'val_loss']].plot()

model_3.summary()

# Commented out IPython magic to ensure Python compatibility.
#Saya hanya melakukan visualisasi pada model terakhir menggunakan tensor board
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs/fit

"""****

# Evaluasi Model dengan Test Data

#### Definisikan fungsi-fungsi untuk memproses hasil uji.
"""

# Buat fungsi untuk mengubah array prediksi menjadi klasifikasi, bukan probabilitas.
def prediction_class(preds):

    for i in range(len(preds)):
        # Find max class probability and change to 1
        preds[i, tf.argmax(preds[i])] = 1
        # Round the rest of the values to 0
        preds[i] = tf.round(preds[i])

    return preds

# Buat fungsi untuk mengubah output numerik menjadi output string.

def numeric_to_class_names(preds, class_names=class_names):
    preds_names = []
    for i in range(len(preds)):
        preds_names.append(class_names[tf.argmax(preds[i])])

    return preds_names

preds1_3 = model_3.predict(X_test_ds)

preds_names1_3 = numeric_to_class_names(preds1_3)
preds_namestest = numeric_to_class_names(y_test_ds)

test_df = pd.DataFrame(data={'Model_3_preds': preds_names1_3, 'Test_Labels': preds_namestest})
test_df.head()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Prediksi model dan label asli (sesuaikan dengan nama data Anda)
y_true = test_df['Test_Labels']  # Label asli
y_pred = test_df['Model_3_preds']  # Prediksi model

# 2. Menghitung Metrik Evaluasi
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred, average='weighted')  # Gunakan 'weighted' untuk multi-kelas
recall = recall_score(y_true, y_pred, average='weighted')
f1 = f1_score(y_true, y_pred, average='weighted')

# 3. Menghitung Confusion Matrix
cm = confusion_matrix(y_true, y_pred)
cm_df = pd.DataFrame(cm,
                     index = ['Non Demented', 'Very Mild Demented', 'Mild Demented', 'Moderate Demented'],
                     columns = ['Non Demented', 'Very Mild Demented', 'Mild Demented', 'Moderate Demented'])

# 4. Menyusun Tabel Metrik
metrics = {
    'Metric': ['Accuracy', 'Precision (Weighted)', 'Recall (Weighted)', 'F1-Score (Weighted)'],
    'Score': [accuracy, precision, recall, f1]
}

metrics_df = pd.DataFrame(metrics)

# Menampilkan tabel metrik dengan rapi
print("\nModel Evaluation Metrics\n")
print(metrics_df.to_string(index=False))
print("\n\n\n")

# 5. Menampilkan Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_df, annot=True, fmt='g', cmap='Blues')
plt.title('Confusion Matrix')
plt.show()
print("\n\n")

# Menyimpan model dalam format .h5
model_3.save('Tugas-2-Kecerdasan-Artifisial-2208107010022.h5')

"""****

# **Analyses and Conclusion**

Custom Convolutional Neural Network (CNN) menunjukkan hasil yang menjanjikan dengan kinerja yang memadai dalam waktu yang relatif singkat. Namun, model ini mengalami sedikit overfitting, yang menunjukkan adanya ruang untuk perbaikan. Kekurangan data pada kelas 'Moderate Demented' menjadi tantangan, dan meskipun augmentasi data dipertimbangkan, hal tersebut dianggap berisiko karena dapat mempengaruhi akurasi kelas. Ke depan, diperlukan optimisasi lebih lanjut, termasuk penyetelan hyperparameter, untuk meningkatkan generalisasi dan ketahanan model. Selain itu, pengumpulan lebih banyak data dan pendalaman pengetahuan domain dapat membantu memaksimalkan penggunaan dataset yang ada. Langkah selanjutnya yang bisa dipertimbangkan adalah pengembangan antarmuka yang ramah pengguna untuk implementasi model.

****

# **Referensi**

`@dataset{alzheimer_mri_dataset,
  author = {Falah.G.Salieh},
  title = {Alzheimer MRI Dataset},
  year = {2023},
  publisher = {Hugging Face},
  version = {1.0},
  url = {https://huggingface.co/datasets/Falah/Alzheimer_MRI}
}`

https://medium.com/geekculture/eda-for-image-classification-dcada9f2567a

https://insightsimaging.springeropen.com/articles/10.1007/s13244-018-0639-9

https://towardsdatascience.com/loss-functions-and-their-use-in-neural-networks-a470e703f1e9

https://insightsimaging.springeropen.com/articles/10.1007/s13244-018-0639-9

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8321322/

https://medium.com/@thecybermarty/multi-class-activation-functions-df969651d4c5#:~:text=Softmax%20is%20the%20most%20commonly,make%20decisions%20based%20on%20them.

https://www.analyticsvidhya.com/blog/2021/05/tuning-the-hyperparameters-and-layers-of-neural-network-deep-learning/
"""